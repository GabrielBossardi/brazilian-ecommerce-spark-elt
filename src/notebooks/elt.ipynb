{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plan\n",
    "- [X] Set up the environment\n",
    "- [X] Load local CSV files\n",
    "- [ ] Upload data to an S3 bucket\n",
    "- [ ] Read data from the S3 bucket\n",
    "- [ ] Write data to S3 in Parquet format with partitioning\n",
    "- [ ] Perform upserts to S3 in Parquet format\n",
    "- [X] Write data to S3 in Delta Table format\n",
    "- [ ] Perform upserts to S3 in Delta Table format\n",
    "- [ ] Load local JSON files\n",
    "- [ ] Write JSON data to S3 in Delta Table format\n",
    "- [X] Practice PySpark transformations and adhere to style guides\n",
    "- [ ] Practice Repartition and Coalesce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# SparkSession Definition\n",
    "APP_NAME = 'Brazilian-Ecommerce'\n",
    "spark = (SparkSession\n",
    "        .builder\n",
    "        .appName(APP_NAME)\n",
    "        .master(\"local[*]\")\n",
    "        .config('spark.jars.packages', 'org.apache.hadoop:hadoop-aws:3.3.2,io.delta:delta-spark_2.12:3.3.0')\n",
    "        .config('spark.hadoop.fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem')\n",
    "        .config('spark.hadoop.fs.s3a.aws.credentials.provider', 'com.amazonaws.auth.DefaultAWSCredentialsProviderChain')  # This will check .aws/credentials\n",
    "        .config('spark.sql.extensions', 'io.delta.sql.DeltaSparkSessionExtension')\n",
    "        .config('spark.sql.catalog.spark_catalog', 'org.apache.spark.sql.delta.catalog.DeltaCatalog')\n",
    "        .getOrCreate()\n",
    "    )\n",
    "\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method SparkSession.stop of <pyspark.sql.session.SparkSession object at 0x107913010>>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Terminate Spark Session\n",
    "spark.stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load local CSV files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "input_folder = '../../storage/raw'\n",
    "df_customers = spark.read.csv(f'{input_folder}/olist_customers_dataset.csv', inferSchema=True, header=True)\n",
    "df_geolocation = spark.read.csv(f'{input_folder}/olist_geolocation_dataset.csv', inferSchema=True, header=True)\n",
    "df_order_items = spark.read.csv(f'{input_folder}/olist_order_items_dataset.csv', inferSchema=True, header=True)\n",
    "df_order_payments = spark.read.csv(f'{input_folder}/olist_order_payments_dataset.csv', inferSchema=True, header=True)\n",
    "df_order_reviews = spark.read.csv(f'{input_folder}/olist_order_reviews_dataset.csv', inferSchema=True, header=True)\n",
    "df_orders = spark.read.csv(f'{input_folder}/olist_orders_dataset.csv', inferSchema=True, header=True)\n",
    "df_products = spark.read.csv(f'{input_folder}/olist_products_dataset.csv', inferSchema=True, header=True)\n",
    "df_products = spark.read.csv(f'{input_folder}/olist_products_dataset.csv', inferSchema=True, header=True)\n",
    "df_sellers = spark.read.csv(f'{input_folder}/olist_sellers_dataset.csv', inferSchema=True, header=True)\n",
    "df_product_category = spark.read.csv(f'{input_folder}/product_category_name_translation.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write data to S3 in Delta Table format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'gb-delta-tables-test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully wrote to S3!\n"
     ]
    }
   ],
   "source": [
    "base_path = f\"s3a://{bucket}/delta\"\n",
    "\n",
    "try:\n",
    "    (df_customers.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .save(f\"{base_path}/customers\")\n",
    "    )\n",
    "    print(\"Successfully wrote to S3!\")\n",
    "except Exception as e:\n",
    "    print(\"Error writing to S3:\", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully wrote to S3!\n"
     ]
    }
   ],
   "source": [
    "bucket = 'gb-delta-tables-test'\n",
    "base_path = f\"s3a://{bucket}/delta\"\n",
    "table_path = f\"{base_path}/order_items\"\n",
    "\n",
    "df_order_items = spark.read.csv(f'{input_folder}/olist_order_items_dataset.csv', inferSchema=True, header=True)\n",
    "\n",
    "try:\n",
    "    (df_order_items.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .save(table_path)\n",
    "    )\n",
    "    print(\"Successfully wrote to S3!\")\n",
    "except Exception as e:\n",
    "    print(\"Error writing to S3:\", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'gb-delta-tables-test'\n",
    "base_path = f\"s3a://{bucket}/delta\"\n",
    "table_path = f\"{base_path}/order_items\"\n",
    "\n",
    "df_order_items_2 = spark.read.csv(f'{input_folder}/olist_order_items_dataset_2.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  112640|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format('delta').load(table_path).createOrReplaceTempView('order_items')\n",
    "spark.sql('SELECT COUNT(*) AS ct FROM order_items').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "deltaTable = DeltaTable.forPath(spark, table_path)\n",
    "\n",
    "merge_condition = 'target.order_id = source.order_id and target.order_item_id = source.order_item_id'\n",
    "\n",
    "(deltaTable.alias('target')\n",
    "    .merge(\n",
    "        df_order_items_2.alias('source'),\n",
    "        merge_condition\n",
    "    )\n",
    "    .whenMatchedUpdateAll()\n",
    "    .whenNotMatchedInsertAll()\n",
    "    .execute()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|    ct|\n",
      "+------+\n",
      "|112650|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format('delta').load(table_path).createOrReplaceTempView('order_items_2')\n",
    "spark.sql('SELECT COUNT(*) AS ct FROM order_items_2').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice PySpark transformations and adhere to style guides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+--------------------+--------------------+-------------------+------+-------------+------------------+\n",
      "|            order_id|order_item_id|          product_id|           seller_id|shipping_limit_date| price|freight_value|shipping_limit_day|\n",
      "+--------------------+-------------+--------------------+--------------------+-------------------+------+-------------+------------------+\n",
      "|00010242fe8c5a6d1...|            1|4244733e06e7ecb49...|48436dade18ac8b2b...|2017-09-19 09:45:35|  58.9|        13.29|        2017-09-19|\n",
      "|00018f77f2f0320c5...|            1|e5f2d52b802189ee6...|dd7ddc04e1b6c2c61...|2017-05-03 11:05:13| 239.9|        19.93|        2017-05-03|\n",
      "|000229ec398224ef6...|            1|c777355d18b72b67a...|5b51032eddd242adc...|2018-01-18 14:48:30| 199.0|        17.87|        2018-01-18|\n",
      "|00024acbcdf0a6daa...|            1|7634da152a4610f15...|9d7a1d34a50524090...|2018-08-15 10:10:18| 12.99|        12.79|        2018-08-15|\n",
      "|00042b26cf59d7ce6...|            1|ac6c3623068f30de0...|df560393f3a51e745...|2017-02-13 13:57:51| 199.9|        18.14|        2017-02-13|\n",
      "|00048cc3ae777c65d...|            1|ef92defde845ab845...|6426d21aca402a131...|2017-05-23 03:55:27|  21.9|        12.69|        2017-05-23|\n",
      "|00054e8431b9d7675...|            1|8d4f2bb7e93e6710a...|7040e82f899a04d1b...|2017-12-14 12:10:31|  19.9|        11.85|        2017-12-14|\n",
      "|000576fe39319847c...|            1|557d850972a7d6f79...|5996cddab893a4652...|2018-07-10 12:30:45| 810.0|        70.75|        2018-07-10|\n",
      "|0005a1a1728c9d785...|            1|310ae3c140ff94b03...|a416b6a846a117243...|2018-03-26 18:31:29|145.95|        11.65|        2018-03-26|\n",
      "|0005f50442cb953dc...|            1|4535b0e1091c278df...|ba143b05f0110f0dc...|2018-07-06 14:10:56| 53.99|         11.4|        2018-07-06|\n",
      "|00061f2a7bc09da83...|            1|d63c1011f49d98b97...|cc419e0650a3c5ba7...|2018-03-29 22:28:09| 59.99|         8.88|        2018-03-29|\n",
      "|00063b381e2406b52...|            1|f177554ea93259a5b...|8602a61d680a10a82...|2018-07-31 17:30:39|  45.0|        12.98|        2018-07-31|\n",
      "|0006ec9db01a64e59...|            1|99a4788cb24856965...|4a3ca9315b744ce9f...|2018-07-26 17:24:20|  74.0|        23.32|        2018-07-26|\n",
      "|0008288aa423d2a3f...|            1|368c6c730842d7801...|1f50f920176fa81da...|2018-02-21 02:55:52|  49.9|        13.37|        2018-02-21|\n",
      "|0008288aa423d2a3f...|            2|368c6c730842d7801...|1f50f920176fa81da...|2018-02-21 02:55:52|  49.9|        13.37|        2018-02-21|\n",
      "|0009792311464db53...|            1|8cab8abac59158715...|530ec6109d11eaaf8...|2018-08-17 12:15:10|  99.9|        27.65|        2018-08-17|\n",
      "|0009c9a17f916a706...|            1|3f27ac8e699df3d30...|fcb5ace8bcc92f757...|2018-05-02 09:31:53| 639.0|        11.34|        2018-05-02|\n",
      "|000aed2e25dbad2f9...|            1|4fa33915031a8cde0...|fe2032dab1a61af87...|2018-05-16 20:57:03| 144.0|         8.77|        2018-05-16|\n",
      "|000c3e6612759851c...|            1|b50c950aba0dcead2...|218d46b86c1881d02...|2017-08-21 03:33:13|  99.0|        13.71|        2017-08-21|\n",
      "|000e562887b1f2006...|            1|5ed9eaf534f6936b5...|8cbac7e12637ed9cf...|2018-02-28 12:08:37|  25.0|        16.11|        2018-02-28|\n",
      "+--------------------+-------------+--------------------+--------------------+-------------------+------+-------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_order_items.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_order_items_filtered = (df_order_items\n",
    "        .filter(F.col('product_id').like('8d4f2bb7e93e6710a%'))\n",
    "        .withColumn('shipping_limit_month', F.date_trunc('month', 'shipping_limit_date'))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_by_month_product = (df_order_items_filtered\n",
    "    .groupBy(\n",
    "        'product_id',\n",
    "        'shipping_limit_month'\n",
    "    )\n",
    "    .agg(F.sum('price').alias('total_price')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+--------------------+--------------------+-------------------+-------------+-----+-----------+\n",
      "|            order_id|order_item_id|          product_id|           seller_id|shipping_limit_date|freight_value|price|price_ratio|\n",
      "+--------------------+-------------+--------------------+--------------------+-------------------+-------------+-----+-----------+\n",
      "|00054e8431b9d7675...|            1|8d4f2bb7e93e6710a...|7040e82f899a04d1b...|2017-12-14 12:10:31|        11.85| 19.9|     0.5559|\n",
      "|2df5ebb637dfe6f39...|            1|8d4f2bb7e93e6710a...|7040e82f899a04d1b...|2018-02-07 02:10:33|        16.79| 19.9|        1.0|\n",
      "|905bd749100612f28...|            1|8d4f2bb7e93e6710a...|7040e82f899a04d1b...|2017-12-04 03:33:17|         15.1| 15.9|     0.4441|\n",
      "+--------------------+-------------+--------------------+--------------------+-------------------+-------------+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "price_by_month_product = price_by_month_product.alias('pbmp')\n",
    "df_order_items_filtered = df_order_items_filtered.alias('doi')\n",
    "\n",
    "join_conditions = [\n",
    "    price_by_month_product.product_id == df_order_items_filtered.product_id,\n",
    "    price_by_month_product.shipping_limit_month == df_order_items_filtered.shipping_limit_month\n",
    "]\n",
    "(price_by_month_product\n",
    "    .join(df_order_items_filtered, on=join_conditions, how='inner')\n",
    "    .select(\n",
    "        F.col('doi.order_id').alias('order_id'),\n",
    "        F.col('doi.order_item_id').alias('order_item_id'),\n",
    "        F.col('doi.product_id').alias('product_id'),\n",
    "        F.col('doi.seller_id').alias('seller_id'),\n",
    "        F.col('doi.shipping_limit_date').alias('shipping_limit_date'),\n",
    "        F.col('doi.freight_value').alias('freight_value'),\n",
    "        F.col('doi.price').alias('price'),\n",
    "        F.col('pbmp.total_price').alias('total_price')\n",
    "    )\n",
    "    .withColumn('price_ratio', F.round(F.col('price') / F.col('total_price'), 4))\n",
    "    .drop('total_price')\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+---+\n",
      "|order_id|order_item_id| ct|\n",
      "+--------+-------------+---+\n",
      "+--------+-------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_order_items.groupBy('order_id', 'order_item_id').agg(F.count('*').alias('ct')).orderBy(F.desc('ct')).filter(F.col('ct') > 1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+-----------+\n",
      "|            order_id|order_item_id|          product_id|           seller_id|shipping_limit_date|price|freight_value|price_ratio|\n",
      "+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+-----------+\n",
      "|00054e8431b9d7675...|            1|8d4f2bb7e93e6710a...|7040e82f899a04d1b...|2017-12-14 12:10:31| 19.9|        11.85|     0.5559|\n",
      "|905bd749100612f28...|            1|8d4f2bb7e93e6710a...|7040e82f899a04d1b...|2017-12-04 03:33:17| 15.9|         15.1|     0.4441|\n",
      "|2df5ebb637dfe6f39...|            1|8d4f2bb7e93e6710a...|7040e82f899a04d1b...|2018-02-07 02:10:33| 19.9|        16.79|        1.0|\n",
      "+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Window as W\n",
    "\n",
    "w_month_price = (W.partitionBy(\n",
    "    'product_id',\n",
    "    F.date_trunc('month', 'shipping_limit_date')\n",
    "))\n",
    "\n",
    "(df_order_items\n",
    "    .filter(F.col('product_id').like('8d4f2bb7e93e6710a%'))\n",
    "    .withColumn('total_price', F.sum('price').over(w_month_price))\n",
    "    .withColumn('price_ratio', F.round(F.col('price') / F.col('total_price'), 4))\n",
    "    .drop('total_price')\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
